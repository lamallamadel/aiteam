# ============================================================================
# Shadow Mode — Atlasia AI Pipeline
# ============================================================================
# Run the agent pipeline on live data alongside the current process, but do
# NOT show outputs to users. Compare agent performance (Pass@1) against the
# human baseline to verify agents are ready for the spotlight.
# ============================================================================

version: "1.0"

# ---------------------------------------------------------------------------
# Overview
# ---------------------------------------------------------------------------
overview:
  description: >
    Shadow Mode is a pre-production validation strategy. The agent pipeline
    processes real GitHub issues in parallel with the human team, but its
    outputs (PRs, reviews, docs) are NEVER merged or shown to users. Instead,
    outputs are stored in a shadow workspace for comparison.

    This validates:
    1. Do agents produce correct code? (Compare against human solution)
    2. Do agents catch the same bugs in review? (Compare findings)
    3. Do agents respect guardrails on real-world data?
    4. What is the real-world Pass@1 rate?

  duration: "2-4 weeks before going live"
  exit_criteria:
    - "Pass@1 rate ≥ 60% on real issues"
    - "No critical guardrail violations in 50+ shadow runs"
    - "Agent cost per issue ≤ 2x human cost threshold"
    - "Zero secret exposure incidents"

# ---------------------------------------------------------------------------
# Execution Model
# ---------------------------------------------------------------------------
execution:
  trigger:
    event: "GitHub issue labeled 'ai:shadow'"
    description: >
      When an issue is labeled, the shadow pipeline runs in parallel with
      the human developer. The shadow pipeline uses a separate branch
      namespace (shadow/*) and never pushes to the main branch.

  isolation:
    branch_prefix: "shadow/"
    pr_target: "shadow-baseline"
    description: >
      Shadow runs create branches like shadow/issue-123 and PRs targeting
      a shadow-baseline branch (not main). This ensures zero impact on the
      production codebase.
    guardrails:
      - "Shadow runs CANNOT push to main, master, or production"
      - "Shadow runs CANNOT merge PRs"
      - "Shadow runs CANNOT post comments visible to external users"
      - "Shadow runs have read-only access to production data"

  resource_limits:
    max_concurrent_shadow_runs: 3
    max_cost_per_shadow_run_usd: 3.00
    max_daily_shadow_budget_usd: 30.00
    timeout_per_run_ms: 900000

# ---------------------------------------------------------------------------
# Comparison Protocol
# ---------------------------------------------------------------------------
comparison:
  description: >
    After both the human and shadow pipeline complete work on an issue,
    compare their outputs across multiple dimensions.

  dimensions:
    code_correctness:
      description: "Does the shadow code compile, pass tests, and solve the issue?"
      method: "automated"
      checks:
        - "Shadow code compiles without errors"
        - "Shadow tests pass"
        - "Shadow PR addresses all acceptance criteria from ticket_plan"
      scoring: "binary (pass/fail) + Judge rubric score"

    code_similarity:
      description: "How similar is the shadow solution to the human solution?"
      method: "automated + llm_judge"
      checks:
        - "Files changed overlap (Jaccard similarity)"
        - "Approach similarity (LLM comparison)"
        - "Test coverage comparison"
      scoring: "0.0 — 1.0 similarity score"

    review_accuracy:
      description: "Did the shadow review catch the same issues a human would?"
      method: "llm_judge"
      checks:
        - "True positive rate (issues found by both)"
        - "False positive rate (shadow found, human didn't)"
        - "False negative rate (human found, shadow missed)"
      scoring: "precision, recall, F1"

    time_comparison:
      description: "How fast is the shadow pipeline vs human?"
      method: "automated"
      checks:
        - "Shadow duration vs human duration"
        - "Shadow token cost vs estimated human cost"

    guardrail_compliance:
      description: "Did the shadow pipeline trigger any guardrail violations?"
      method: "automated"
      checks:
        - "Zero critical interrupt triggers"
        - "Zero blackboard access violations"
        - "Zero secret exposure attempts"
        - "All artifacts schema-valid"

# ---------------------------------------------------------------------------
# Swiss Cheese Defense Validation
# ---------------------------------------------------------------------------
swiss_cheese:
  description: >
    Shadow mode validates the multi-layer "Swiss Cheese" defense model.
    Each layer catches errors the previous layer missed. No single layer
    is perfect, but together they provide comprehensive coverage.

  layers:
    - layer: "Schema Validation"
      description: "Every artifact validated against JSON schema"
      failure_signal: "SchemaValidation event with valid=false"
      coverage: "Structural correctness of all inter-agent data"

    - layer: "Blackboard Access Control"
      description: "Agents can only read/write authorized entries"
      failure_signal: "BlackboardAccessException"
      coverage: "Data isolation between agents"

    - layer: "Dynamic Interrupts"
      description: "Real-time tool call monitoring with tiered blocking"
      failure_signal: "InterruptDecision with action != PROCEED"
      coverage: "Destructive operations, secret exposure, scope violations"

    - layer: "Persona Review (Critic)"
      description: "Multi-persona code review with conflict resolution"
      failure_signal: "persona_review.status = 'rejected'"
      coverage: "Security, quality, infrastructure, UX issues"

    - layer: "Judge Evaluation"
      description: "Independent quality arbiter with veto power"
      failure_signal: "judge_verdict.verdict = 'veto'"
      coverage: "Overall quality below threshold"

    - layer: "Majority Voting"
      description: "3-voter consensus for high-stakes decisions"
      failure_signal: "voting_metadata.agreement_rate < 0.67"
      coverage: "Reduces single-evaluator bias"

    - layer: "HITL Gates"
      description: "Human approval at architecture, PR, and merge gates"
      failure_signal: "gate_decision = 'reject'"
      coverage: "Final human judgment on critical decisions"

  validation:
    description: >
      For each shadow run, record which layers caught issues. Compute
      per-layer catch rate to identify weak layers that need strengthening.
    metric: "orchestrator.swiss_cheese.layer.catch_rate (gauge, tagged by layer)"

# ---------------------------------------------------------------------------
# Reporting
# ---------------------------------------------------------------------------
reporting:
  cadence: "Daily summary + per-issue detailed reports"

  daily_summary:
    metrics:
      - "Shadow Pass@1 rate (last 7 days)"
      - "Shadow vs Human code similarity (average)"
      - "Shadow vs Human review F1 score"
      - "Guardrail violation count"
      - "Total shadow cost (USD)"
      - "Average shadow duration vs human duration"
    format: "Markdown report + dashboard update"

  per_issue_report:
    fields:
      - "Issue ID and title"
      - "Shadow outcome (completed/escalated/failed)"
      - "Human outcome"
      - "Code correctness (pass/fail)"
      - "Judge verdict and score"
      - "Files changed comparison"
      - "Review findings comparison"
      - "Duration and cost comparison"
      - "Guardrail events"
    format: "JSON (eval_result.schema.json) + Markdown"

  go_live_report:
    description: >
      Final report generated at the end of the shadow period summarizing
      overall readiness. Requires human sign-off before enabling live mode.
    checklist:
      - "Pass@1 rate ≥ 60% on real issues"
      - "Zero critical guardrail violations"
      - "Review F1 score ≥ 0.70"
      - "Average cost per issue ≤ $2.00"
      - "No secret exposure incidents"
      - "Human sign-off obtained"
